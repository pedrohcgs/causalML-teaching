---
title: "Supervised ML: Lasso saves the job of OLS"
author: "Michael Knaus"
date: "`r format(Sys.time(), '%m/%y')`"
output: 
  html_notebook:
    toc: true
    toc_float: true
    code_folding: show
---

<br>

Goals:

- Illustrate how Lasso works

- Illustrate that Lasso does not overfit

<br>

## Data generating process

We use the same DGP as in the notebook [Overfitting of OLS and value of training vs. test sample](https://mcknaus.github.io/assets/notebooks/SNB/SNB_OLS_in_vs_out_of_sample.nb.html):

- $p\geq10$ independent and standard normal covariates: $X \sim N(0,I_p)$, where $I_p$ is the $p$-dimensional identity matrix

- The outcome model is $Y = \underbrace{\beta_0 + \beta_1 X_1 + ... + \beta_{10} X_{10}}_{\text{conditional expectation function }m(X)} + e$, where $X_j$ is the $j$-th column of $X$ and $e \sim N(0,1)$

- We consider the following parameters $\beta_0 = 0$, $\beta_1 = 1$, $\beta_2 = 0.9$, ..., $\beta_9 = 0.2$, $\beta_{10} = 0.1$ such that the first 10 variables have a decreasing impact on the outcome and any further variables are just irrelevant noise with no true predictive power

We set $p=99$ and draw a training sample with $N_{tr}=100$ and a test sample with $N_{te}=10,000$

```{r message=FALSE, warning = FALSE}
# Load the packages required for later
if (!require("tidyverse")) install.packages("tidyverse", dependencies = TRUE); library(tidyverse)
if (!require("glmnet")) install.packages("glmnet", dependencies = TRUE); library(glmnet)
if (!require("hdm")) install.packages("hdm", dependencies = TRUE); library(hdm)
if (!require("plasso")) install.packages("plasso", dependencies = TRUE); library(plasso)

set.seed(1234) # For replicability

# Define the important parameters
n_tr = 100
n_te = 10000
p = 99
beta = c(0,seq(1,0.1,-0.1),rep(0,p-10))

# Combine constant and randomly drawn covariates
x_tr = cbind(rep(1,n_tr),matrix(rnorm(n_tr*p),ncol=p))
x_te = cbind(rep(1,n_te),matrix(rnorm(n_te*p),ncol=p))

# Create the CEF using matrix multiplication for compactness
cfe_tr = x_tr %*% beta
cfe_te = x_te %*% beta

# Create the "observed" outcomes by adding noise
y_tr = cfe_tr + rnorm(n_tr,0,1)
y_te = cfe_te + rnorm(n_te,0,1)
```

<br>

## Lasso at work

We run Lasso with all 99 regressors. The `glmnet` command calculates the trajectory from an empty model to a nearly unpenalized model (read from right to left):

```{r}
# As most of the commands that we use from now on, 
# glmnet takes inputs in matrix form and not as formulas
lasso = glmnet(x_tr,y_tr)
plot(lasso, xvar = "lambda")
```

In the beginning the coefficients of the really relevant variables are build up and at some point the excessive overfitting starts. 

**Cross-validation** helps to figure out the sweet spot that ensures a good out-of-sample performance. The `cv.glmnet` command uses by default 10-fold cross-validation leading to the following result:

```{r}
cv_lasso = cv.glmnet(x_tr,y_tr)
plot(cv_lasso)
```

We observe that the cross-validated MSE decreases as the relevant variables are selected and build up. At some point the reduced penalization leads to selection of noise variables and the cross-validated MSE deteriorated again. We select the penalty term where the curve indicates the lowest MSE (alternative is the 1SE rule indicated by the right dashed line, but we don't go into the details here).

<br>

## Post-Lasso at work

### Cross-validated

Cross-validation for Post-Lasso is implemented in the `plasso` package:

```{r}
# Provide the cov matrix w/o the constant x_tr[,-1]
# Increasing lambda.min.ratio ensures that Lasso does not overfit too heavily and reduces running time
post_lasso = plasso(x_tr[,-1],y_tr,lambda.min.ratio=0.01)
plot(post_lasso,xvar = "lambda")
```

While Lasso builds up coefficients gradually, Post-Lasso gives the full OLS coefficient as soon as the variable is selected $\Rightarrow$ the coefficients paths of Post-Lasso are usually not smooth.

This is also observable in the cross-validation curve:


```{r}
cv_plasso = cv.plasso(x_tr,y_tr,lambda.min.ratio=0.01)
plot(cv_plasso, legend_pos="bottomleft")
```

The comparison of Lasso and Post-Lasso cross-validation is instructive to understand some differences:

- The Post-Lasso cross-validation curve is more bumpy and has flat regions compared to the Lasso curve. Post-Lasso gives the full OLS coefficient as soon as the variable is selected. This explains the flat regions of the Post-Lasso curve where no additional variable is selected and thus MSE stays constant.

- Post-Lasso usually produces sparser cross-validated models because the unshrunken coefficients deliver more explanatory power with a smaller set of variables.

<br>

### Fast implementation with `hdm` 

While the cross-validated Post-Lasso is instructive, it is not fast and the `rlasso` function of the `hdm` package implements a faster way to choose the penalty parameter in a data-driven way:

```{r}
post_hdm = rlasso(x_tr,y_tr)
summary(post_hdm)
```


<br>
<br>

## OLS vs. (Post-)Lasso

We replicate the analysis of Notebook [Overfitting of OLS and value of training vs. test sample](https://mcknaus.github.io/assets/notebooks/SNB/SNB_OLS_in_vs_out_of_sample.nb.html) and gradually add covariates to check how out-of-sample performance develops.

```{r}
# Container of the results
results_ols = results_lasso = results_plasso = results_rlasso = matrix(NA,p-1,4)
colnames(results_ols) = colnames(results_lasso) = 
                        colnames(results_plasso) = 
                        colnames(results_rlasso) = c("Obs MSE train","Obs MSE test",
                                                    "Oracle MSE train","Oracle MSE test")

# Loop that gradually adds variables (start with 2, otherwise glmnet crashes)
for (i in 2:p) {
  # OLS
  temp_ols = lm(y_tr ~ x_tr[,2:(i+1)])
  temp_yhat_tr = predict(temp_ols)
  temp_yhat_te = x_te[,1:(i+1)] %*% temp_ols$coefficients
  # Calculate the observable MSEs in training and test sample
  results_ols[i-1,1] = mean((y_tr - temp_yhat_tr)^2) # in-sample MSE
  results_ols[i-1,2] = mean((y_te - temp_yhat_te)^2) # out-of-sample MSE
  # Calculate the oracle MSEs that are only observables b/c we know the true CEF
  results_ols[i-1,3] = var(y_tr - cfe_tr) + mean((cfe_tr - temp_yhat_tr)^2)
  results_ols[i-1,4] = var(y_te - cfe_te) + mean((cfe_te - temp_yhat_te)^2)
  
  # Lasso
  temp_lasso = cv.glmnet(x_tr[,2:(i+1)],y_tr)
  temp_yhat_tr = predict(temp_lasso,newx=x_tr[,2:(i+1)])
  temp_yhat_te = predict(temp_lasso,newx=x_te[,2:(i+1)])
  # Calculate the observable MSEs in training and test sample
  results_lasso[i-1,1] = mean((y_tr - temp_yhat_tr)^2) # in-sample MSE
  results_lasso[i-1,2] = mean((y_te - temp_yhat_te)^2) # out-of-sample MSE
  # Calculate the oracle MSEs that are only observables b/c we know the true CEF
  results_lasso[i-1,3] = var(y_tr - cfe_tr) + mean((cfe_tr - temp_yhat_tr)^2)
  results_lasso[i-1,4] = var(y_te - cfe_te) + mean((cfe_te - temp_yhat_te)^2)
  
  # plasso
  temp_plasso = cv.plasso(x_tr[,2:(i+1)],y_tr)
  temp_yhat_tr = predict(temp_plasso,newx=x_tr[,2:(i+1)])$plasso
  temp_yhat_te = predict(temp_plasso,newx=x_te[,2:(i+1)])$plasso
  # Calculate the observable MSEs in training and test sample
  results_plasso[i-1,1] = mean((y_tr - temp_yhat_tr)^2) # in-sample MSE
  results_plasso[i-1,2] = mean((y_te - temp_yhat_te)^2) # out-of-sample MSE
  # Calculate the oracle MSEs that are only observables b/c we know the true CEF
  results_plasso[i-1,3] = var(y_tr - cfe_tr) + mean((cfe_tr - temp_yhat_tr)^2)
  results_plasso[i-1,4] = var(y_te - cfe_te) + mean((cfe_te - temp_yhat_te)^2)
  
  # rlasso
  temp_rlasso = rlasso(x_tr[,2:(i+1)],y_tr)
  temp_yhat_tr = predict(temp_rlasso,newdata=x_tr[,2:(i+1)])
  temp_yhat_te = predict(temp_rlasso,newdata=x_te[,2:(i+1)])
  # Calculate the observable MSEs in training and test sample
  results_rlasso[i-1,1] = mean((y_tr - temp_yhat_tr)^2) # in-sample MSE
  results_rlasso[i-1,2] = mean((y_te - temp_yhat_te)^2) # out-of-sample MSE
  # Calculate the oracle MSEs that are only observables b/c we know the true CEF
  results_rlasso[i-1,3] = var(y_tr - cfe_tr) + mean((cfe_tr - temp_yhat_tr)^2)
  results_rlasso[i-1,4] = var(y_te - cfe_te) + mean((cfe_te - temp_yhat_te)^2)
}
```

Again, OLS explodes but the Lassos show a stable prediction performance regardless of the number of noise variables.

```{r}
df = data.frame(Estimator = c(rep("OLS",p-1),rep("Lasso",p-1),rep("Post-Lasso CV",p-1),rep("Post-Lasso hdm",p-1)),
                Number.of.variables = c(2:p,2:p,2:p,2:p),
                Obs.MSE.test = c(results_ols[,2], results_lasso[,2], results_plasso[,2], results_rlasso[,2]),
                Oracle.MSE.test = c(results_ols[,4], results_lasso[,4], results_plasso[,4],results_plasso[,4]))

ggplot(subset(df), aes(x=Number.of.variables,y=Obs.MSE.test,colour=Estimator)) + geom_line(size=1)

ggplot(subset(df,df$Number.of.variables<70), aes(x=Number.of.variables,y=Obs.MSE.test,colour=Estimator))     + geom_line(size=1)  + geom_hline(yintercept = 0)
```

It seems that Post-Lasso has a slight advantage over plain Lasso. However, this is DGP dependent and could be flipped in other settings.

<br>
<br>

### Take-away
 
 - The penalization of coefficients ensures good out-of-sample performance $\Rightarrow$ Lasso works
 
<br>
<br>
 
 
### Suggestions to play with the toy model

Feel free to play around with the code. This is useful to sharpen and challenge your understanding of the methods. Think about the consequences of a modifications before you run it and check whether the results are in line with your expectation. Some suggestions:
 
- Modify DGP (change betas, change level of noise, introduce correlation between covariates, ...)

- Modify seed

- Change training and test sample size

- Investigate the 1SE rule for cross-validated Lasso
 

 